{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec5216ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0ba364",
   "metadata": {},
   "source": [
    "creating a normalized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78e1ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from C:\\Users\\Cheig\\capstone\\dh_stuff\\SHADXWBXRN_analysis_20250331_173555\\data\\dh_data.csv...\n",
      "Converting Estimated_Key to numerical values...\n",
      "Removing Title, Artist, and Album columns...\n",
      "Normalizing numerical columns...\n",
      "Saving processed dataset to processed_dataset2.csv...\n",
      "Processing complete!\n"
     ]
    }
   ],
   "source": [
    "# File paths\n",
    "input_file = \"C:\\\\Users\\\\Cheig\\\\OneDrive\\\\Desktop\\\\csvs\\\\data_with_ids.csv\" # Change this to your input file path\n",
    "#input_file = \"C:\\\\Users\\\\Cheig\\\\capstone\\\\dh_stuff\\\\SHADXWBXRN_analysis_20250331_173555\\\\data\\\\dh_data.csv\"\n",
    "output_file = 'processed_dataset2.csv'  # Change this to your desired output file path\n",
    "\n",
    "# Load the dataset\n",
    "print(f\"Loading dataset from {input_file}...\")\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Apply the key_to_number function to convert Estimated_Key to numerical values\n",
    "print(\"Converting Estimated_Key to numerical values...\")\n",
    "df['Estimated_Key'] = df['Estimated_Key'].apply(key_to_number)\n",
    "\n",
    "# Remove Title, Artist, and Album columns\n",
    "print(\"Removing Title, Artist, and Album columns...\")\n",
    "columns_to_remove = ['Title', 'Artist', 'Album']\n",
    "df_processed = df.drop(columns=columns_to_remove)\n",
    "\n",
    "# Normalize each column (excluding song_id)\n",
    "print(\"Normalizing numerical columns...\")\n",
    "scaler = MinMaxScaler()\n",
    "columns_to_normalize = [col for col in df_processed.columns if col != 'song_id']\n",
    "\n",
    "# Handle potential NaN values before normalization\n",
    "for col in columns_to_normalize:\n",
    "    # Fill NaN values with column mean if numerical\n",
    "    if pd.api.types.is_numeric_dtype(df_processed[col]):\n",
    "        mean_val = df_processed[col].mean()\n",
    "        df_processed[col] = df_processed[col].fillna(mean_val)\n",
    "\n",
    "# Separate song_id before normalization\n",
    "song_ids = df_processed['song_id']\n",
    "df_to_normalize = df_processed.drop(columns=['song_id'])\n",
    "\n",
    "# Apply normalization only to numeric columns\n",
    "numeric_columns = df_to_normalize.select_dtypes(include=[np.number]).columns\n",
    "df_to_normalize[numeric_columns] = scaler.fit_transform(df_to_normalize[numeric_columns])\n",
    "\n",
    "# Reattach song_id\n",
    "df_normalized = pd.concat([song_ids, df_to_normalize], axis=1)\n",
    "\n",
    "# Save the processed dataset to a new file\n",
    "print(f\"Saving processed dataset to {output_file}...\")\n",
    "df_normalized.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"Processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "853a7f87-9a03-41df-904a-ddc114f6fea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the key into a numperical feature \n",
    "def key_to_number(key_str):\n",
    "    \"\"\"Convert key strings to numerical values.\n",
    "    Major keys: C=0, C#=1, D=2, ... B=11\n",
    "    Minor keys: Am=12, A#m=13, Bm=14, ... G#m=23\n",
    "    \n",
    "    Handles various notations:\n",
    "    - Full notation: 'C major', 'D minor'\n",
    "    - Short notation: 'C', 'Dm'\n",
    "    - Symbol notation: 'C#', 'F#m'\n",
    "    \"\"\"\n",
    "    if pd.isna(key_str) or key_str == '':\n",
    "        return np.nan\n",
    "    \n",
    "    # Standardize the key string (replace flats with equivalent sharps)\n",
    "    key_str = key_str.replace('Ab', 'G#').replace('Bb', 'A#').replace('Cb', 'B').replace('Db', 'C#').replace('Eb', 'D#')\n",
    "    key_str = key_str.strip()\n",
    "    \n",
    "    # Define all possible notes\n",
    "    notes = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']\n",
    "    \n",
    "    # Case 1: Full notation (e.g., \"C major\", \"D minor\")\n",
    "    full_notation = re.match(r'([A-G][#]?)\\s*(major|minor)$', key_str)\n",
    "    if full_notation:\n",
    "        note, scale = full_notation.groups()\n",
    "        note_idx = notes.index(note) if note in notes else -1\n",
    "        if note_idx >= 0:\n",
    "            if scale == 'major':\n",
    "                return note_idx\n",
    "            else:  # minor\n",
    "                return note_idx + 3 + 12  # Relative minor is 3 semitones up, then add 12 to differentiate\n",
    "    \n",
    "    # Case 2: Short notation with explicit minor (e.g., \"Dm\", \"F#m\")\n",
    "    short_minor = re.match(r'([A-G][#]?)m$', key_str)\n",
    "    if short_minor:\n",
    "        note = short_minor.group(1)\n",
    "        note_idx = notes.index(note) if note in notes else -1\n",
    "        if note_idx >= 0:\n",
    "            return note_idx + 3 + 12  # Minor key\n",
    "    \n",
    "    # Case 3: Just the note name - assume major (e.g., \"C\", \"F#\")\n",
    "    if key_str in notes:\n",
    "        return notes.index(key_str)  # Major key\n",
    "    \n",
    "    print(f\"Could not parse key: {key_str}\")\n",
    "    return np.nan\n",
    "def main():\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(\"C:\\\\Users\\\\Cheig\\\\OneDrive\\\\Desktop\\\\csvs\\\\data_with_ids.csv\")  # Replace with your actual file name\n",
    "    \n",
    "    # Convert Estimated_Key to numerical values\n",
    "    df['Estimated_Key_Numeric'] = df['Estimated_Key'].apply(key_to_number)\n",
    "    \n",
    "    # Identify columns to process (exclude song_id, Title, Artist, Album)\n",
    "    exclude_cols = ['song_id', 'Title', 'Artist', 'Album']\n",
    "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    # Create a dictionary to store ordered song IDs for each feature\n",
    "    ordered_songs_dict = {}\n",
    "    \n",
    "    # For each feature, sort songs and extract ordered list of song_ids\n",
    "    for feature in feature_cols:\n",
    "        # Skip if the feature is non-numeric\n",
    "        if df[feature].dtype == 'object' and feature != 'Estimated_Key':\n",
    "            print(f\"Skipping non-numeric feature: {feature}\")\n",
    "            continue\n",
    "        \n",
    "        # Use the numeric version for Estimated_Key\n",
    "        if feature == 'Estimated_Key':\n",
    "            feature = 'Estimated_Key_Numeric'\n",
    "        \n",
    "        # Sort by feature value and extract song_ids\n",
    "        try:\n",
    "            # Drop NaN values before sorting\n",
    "            sorted_df = df.dropna(subset=[feature]).sort_values(by=feature)\n",
    "            ordered_song_ids = sorted_df['song_id'].tolist()\n",
    "            \n",
    "            # Store in dictionary (using original feature name)\n",
    "            if feature == 'Estimated_Key_Numeric':\n",
    "                ordered_songs_dict['Estimated_Key'] = ordered_song_ids\n",
    "            else:\n",
    "                ordered_songs_dict[feature] = ordered_song_ids\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing feature {feature}: {e}\")\n",
    "    \n",
    "    # Create a new DataFrame for the ordered song IDs\n",
    "    result_df = pd.DataFrame({\n",
    "        'feature': list(ordered_songs_dict.keys()),\n",
    "        'ordered_song_ids': list(ordered_songs_dict.values())\n",
    "    })\n",
    "    \n",
    "    # Save to CSV\n",
    "    result_df.to_csv('ordered_songs_by_feature.csv', index=False)\n",
    "    print(f\"Saved ordered song IDs to ordered_songs_by_feature.csv\")\n",
    "#Create ordered lists of song ids by feature\n",
    "# def main():\n",
    "#     # Read the CSV file\n",
    "#     df = pd.read_csv(\"C:\\\\Users\\\\Cheig\\\\OneDrive\\\\Desktop\\\\csvs\\\\data_with_ids.csv\")  # Replace with your actual file name\n",
    "    \n",
    "#     # Convert Estimated_Key to numerical values\n",
    "#     df['Estimated_Key_Numeric'] = df['Estimated_Key'].apply(key_to_number)\n",
    "    \n",
    "#     # Identify columns to process (exclude song_id, Title, Artist, Album)\n",
    "#     exclude_cols = ['song_id', 'Title', 'Artist', 'Album']\n",
    "#     feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "#     # Create a dictionary to store ordered song IDs and their values for each feature\n",
    "#     ordered_songs_dict = {}\n",
    "    \n",
    "#     # For each feature, sort songs and extract ordered list of song_ids with values\n",
    "#     for feature in feature_cols:\n",
    "#         # Skip if the feature is non-numeric\n",
    "#         if df[feature].dtype == 'object' and feature != 'Estimated_Key':\n",
    "#             print(f\"Skipping non-numeric feature: {feature}\")\n",
    "#             continue\n",
    "        \n",
    "#         # Use the numeric version for Estimated_Key for sorting, but preserve the original key name in output\n",
    "#         if feature == 'Estimated_Key':\n",
    "#             sort_feature = 'Estimated_Key_Numeric'\n",
    "#             feature_to_store = 'Estimated_Key'\n",
    "#         else:\n",
    "#             sort_feature = feature\n",
    "#             feature_to_store = feature\n",
    "        \n",
    "#         # Sort by feature value and extract song_ids with their values\n",
    "#         try:\n",
    "#             # Drop NaN values before sorting\n",
    "#             sorted_df = df.dropna(subset=[sort_feature]).sort_values(by=sort_feature)\n",
    "            \n",
    "#             # Create list of tuples (song_id, feature_value)\n",
    "#             if feature == 'Estimated_Key':\n",
    "#                 # For Estimated_Key, store both original key and numeric value\n",
    "#                 ordered_songs_with_values = [\n",
    "#                     (row['song_id'], (row[feature_to_store], row[sort_feature])) \n",
    "#                     for _, row in sorted_df.iterrows()\n",
    "#                 ]\n",
    "#             else:\n",
    "#                 ordered_songs_with_values = [\n",
    "#                     (row['song_id'], row[feature_to_store]) \n",
    "#                     for _, row in sorted_df.iterrows()\n",
    "#                 ]\n",
    "            \n",
    "#             # Store in dictionary\n",
    "#             ordered_songs_dict[feature_to_store] = ordered_songs_with_values\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing feature {feature}: {e}\")\n",
    "#         print(feature)\n",
    "#     # Create a new DataFrame for the ordered song IDs with values\n",
    "#     result_df = pd.DataFrame({\n",
    "#         'feature': list(ordered_songs_dict.keys()),\n",
    "#         'ordered_song_ids_with_values': list(ordered_songs_dict.values())\n",
    "#     })\n",
    "    \n",
    "#     # Save to CSV\n",
    "#     result_df.to_csv('ordered_songs_by_feature_with_values.csv', index=False)\n",
    "#     print(f\"Saved ordered song IDs with values to ordered_songs_by_feature_with_values.csv\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6203bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "def load_data1(csv_filename):\n",
    "    \"\"\"Load the CSV and parse the ordered song IDs as lists using pandas.\"\"\"\n",
    "    df = pd.read_csv(csv_filename)\n",
    "    data = {row[\"feature\"]: ast.literal_eval(row[\"ordered_song_ids\"]) for _, row in df.iterrows()}\n",
    "    return data\n",
    "def load_data2(csv_filename):\n",
    "    \"\"\"Load the CSV and parse the ordered song IDs with values using pandas.\"\"\"\n",
    "    df = pd.read_csv(csv_filename)\n",
    "    data = {row[\"feature\"]: ast.literal_eval(row[\"ordered_song_ids_with_values\"]) for _, row in df.iterrows()}\n",
    "    return data\n",
    "\n",
    "def load_data3(csv_filename):\n",
    "    # Read the CSV into a DataFrame\n",
    "    df = pd.read_csv(csv_filename)\n",
    "    \n",
    "    # Convert 'Estimated_Key' column to numerical data\n",
    "    df['Estimated_Key'] = df['Estimated_Key'].apply(key_to_number)\n",
    "    \n",
    "    # Drop rows with NaN values in any column\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Exclude non-numeric columns (Title, Artist, and Album) and select the numeric columns\n",
    "    df_numeric = df.drop(columns=['Title', 'Artist', 'Album', 'song_id'])  # Exclude non-numeric and ID columns\n",
    "    \n",
    "    # Feature matrix with only numeric columns\n",
    "    feature_matrix = df_numeric.values\n",
    "    \n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7f020e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = load_data1(\"C:\\\\Users\\\\Cheig\\\\capstone\\\\Spectralify\\\\reccomendation\\\\ordered_songs_by_feature.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "01f0a730",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_numpy = load_data3(\"C:\\\\Users\\\\Cheig\\\\OneDrive\\\\Desktop\\\\csvs\\\\data_with_ids.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ca9966e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common nearby song: [(44141, 20), (3272, 19), (17377, 19), (9102, 18), (15737, 18), (10489, 18), (3198, 17), (46504, 16), (45664, 16), (25878, 16), (3260, 16), (24195, 16), (42179, 16), (1843, 16), (29230, 16), (37719, 16), (23471, 16), (23556, 16), (17845, 15), (41684, 15)]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "#Using ordered lists to find the nearest x songs with the option to use feature groups \n",
    "def find_nearest_songs(data, song_id, num_neighbors=1000, groups = None):\n",
    "    \"\"\"Find the 100 nearest songs for each feature and count occurrences.\"\"\"\n",
    "    song_counts = Counter()\n",
    "\n",
    "    if groups is not None:\n",
    "        mapping = {\n",
    "        \"basic\": [\n",
    "            \"Duration_Seconds\", \"Tempo_BPM\", \"Beat_Regularity\", \"Beat_Density\", \"Beat_Strength\"\n",
    "        ],\n",
    "        \"pitch\": [\n",
    "            \"Estimated_Key\", \"Key_Confidence\", \"Average_Pitch\", \"Pitch_Range\", \"pYIN_Pitch\", \"Harmonic_Salience\"\n",
    "        ],\n",
    "        \"spectral\": [\n",
    "            \"Average_Spectral_Centroid\", \"Average_Spectral_Rolloff\", \"Average_Spectral_Bandwidth\",\n",
    "            \"Spectral_Contrast_Mean\", \"Spectral_Entropy\", \"Spectral_Flatness\", \"Tonnetz_Features\", \"Polynomial_Coefficients\"\n",
    "        ],\n",
    "        \"energy\": [\n",
    "            \"RMS_Energy_Mean\", \"RMS_Energy_Std\", \"Dynamic_Range\", \"Crest_Factor\", \"PCEN_Energy\"\n",
    "        ],\n",
    "        \"harmonic\": [\n",
    "            \"Harmonic_Ratio\", \"Tonal_Energy_Ratio\", \"Variable_Q_Features\", \"Reassigned_Features\"\n",
    "        ],\n",
    "        \"rhythm\": [\n",
    "            \"Groove_Consistency\", \"Pulse_Clarity\", \"Fourier_Tempogram\", \"Tempogram_Ratio\", \"Onset_Rate\", \"Onset_Strength_Mean\"\n",
    "        ],\n",
    "        \"structure\": [\n",
    "            \"RQA_Features\", \"Path_Enhanced_Structure\", \"HPSS_Separation\", \"MultipleSegmentation_Boundaries\"\n",
    "        ]\n",
    "        }\n",
    "    \n",
    "\n",
    "        \n",
    "        columns = []\n",
    "        for keyword in groups:\n",
    "            columns.extend(mapping.get(keyword.lower(), []))\n",
    "        for feature, song_list in data.items():\n",
    "            if feature in columns:\n",
    "                if song_id in song_list:\n",
    "                    index = song_list.index(song_id)\n",
    "                    nearest_songs = song_list[max(0, index - num_neighbors // 2): index + num_neighbors // 2]\n",
    "                    song_counts.update(nearest_songs)\n",
    "    \n",
    "    \n",
    "    else: \n",
    "        for feature, song_list in data.items():\n",
    "            if song_id in song_list:\n",
    "                index = song_list.index(song_id)\n",
    "                nearest_songs = song_list[max(0, index - num_neighbors // 2): index + num_neighbors // 2]\n",
    "                song_counts.update(nearest_songs)\n",
    "    \n",
    "    # Remove the original song ID itself from the count\n",
    "    song_counts.pop(song_id, None)\n",
    "    \n",
    "    # Find the most common nearby song\n",
    "    most_common_song = song_counts.most_common(20) if song_counts else (None, 0)\n",
    "    \n",
    "    return most_common_song\n",
    "\n",
    "# Example usage\n",
    "csv_filename = \"C:\\\\Users\\\\Cheig\\\\capstone\\\\ordered_songs_by_feature.csv\"  # Change this to your actual CSV filename\n",
    "song_id_to_search = 0  # Change this to the song ID you're searching for\n",
    "\n",
    "data = data1\n",
    "most_common_song = find_nearest_songs(data, song_id_to_search )#groups=['basic', 'rhythm']\n",
    "print(f\"Most common nearby song: {most_common_song}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "62b6a730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 15613, 15633, 1001, 46508, 5542, 12842, 17361, 32124, 31987, 17591, 20135, 40055, 38038, 37898, 38092, 38203, 17378, 1787, 31587]\n"
     ]
    }
   ],
   "source": [
    "def centroid_method(feature_matrix, song_ids, num_recommendations=20):\n",
    "    \"\"\"\n",
    "    Finds the closest songs to the centroid of the given songs.\n",
    "    \"\"\"\n",
    "    centroid = np.mean(feature_matrix[song_ids], axis=0)\n",
    "    distances = np.linalg.norm(feature_matrix - centroid, axis=1)\n",
    "    \n",
    "    nearest_indices = np.argsort(distances)\n",
    "    recommended_songs = [idx for idx in nearest_indices if idx not in song_ids][:num_recommendations]\n",
    "    \n",
    "    return recommended_songs\n",
    "\n",
    "print(centroid_method(data_numpy, [1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dbadc3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 5542, 1001, 26497, 15633, 46508, 15613, 20135, 62, 14931, 14583, 36455, 36423, 38139, 38027, 37897, 28308, 17361, 43732, 28914]\n"
     ]
    }
   ],
   "source": [
    "def multi_song_proximity_method(feature_matrix, song_ids, num_recommendations=20):\n",
    "    \"\"\"\n",
    "    Finds songs that are closest to all input songs by summing their distances.\n",
    "    \"\"\"\n",
    "    total_distance = np.sum([np.linalg.norm(feature_matrix - feature_matrix[song], axis=1) for song in song_ids], axis=0)\n",
    "    \n",
    "    nearest_indices = np.argsort(total_distance)\n",
    "    recommended_songs = [idx for idx in nearest_indices if idx not in song_ids][:num_recommendations]\n",
    "    \n",
    "    return recommended_songs\n",
    "\n",
    "print(multi_song_proximity_method(data_numpy, [1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5e3909bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConvexHull\n",
      "File \u001b[1;32mc:\\Users\\Cheig\\anaconda3\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_frame.py:988\u001b[0m, in \u001b[0;36mPyDBFrame.trace_dispatch\u001b[1;34m(self, frame, event, arg)\u001b[0m\n\u001b[0;32m    986\u001b[0m \u001b[38;5;66;03m# if thread has a suspend flag, we suspend with a busy wait\u001b[39;00m\n\u001b[0;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info\u001b[38;5;241m.\u001b[39mpydev_state \u001b[38;5;241m==\u001b[39m STATE_SUSPEND:\n\u001b[1;32m--> 988\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_wait_suspend(thread, frame, event, arg)\n\u001b[0;32m    989\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrace_dispatch\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Cheig\\anaconda3\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_frame.py:165\u001b[0m, in \u001b[0;36mPyDBFrame.do_wait_suspend\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_wait_suspend\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 165\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdo_wait_suspend(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Cheig\\anaconda3\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[1;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[0;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Cheig\\anaconda3\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[0;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[1;32m-> 2106\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[0;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial import ConvexHull\n",
    "import re\n",
    "from collections import Counter\n",
    "import random \n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def apply_pca(feature_matrix, n_components=20):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    return pca.fit_transform(feature_matrix)\n",
    "\n",
    "# Function to convert key strings to numerical values\n",
    "def key_to_number(key_str):\n",
    "    \"\"\"Convert key strings to numerical values.\n",
    "    Major keys: C=0, C#=1, D=2, ... B=11\n",
    "    Minor keys: Am=12, A#m=13, Bm=14, ... G#m=23\n",
    "    \n",
    "    Handles various notations:\n",
    "    - Full notation: 'C major', 'D minor'\n",
    "    - Short notation: 'C', 'Dm'\n",
    "    - Symbol notation: 'C#', 'F#m'\n",
    "    \"\"\"\n",
    "    if pd.isna(key_str) or key_str == '':\n",
    "        return np.nan\n",
    "    \n",
    "    # Standardize the key string (replace flats with equivalent sharps)\n",
    "    key_str = key_str.replace('Ab', 'G#').replace('Bb', 'A#').replace('Cb', 'B').replace('Db', 'C#').replace('Eb', 'D#')\n",
    "    key_str = key_str.strip()\n",
    "    \n",
    "    # Define all possible notes\n",
    "    notes = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']\n",
    "    \n",
    "    # Case 1: Full notation (e.g., \"C major\", \"D minor\")\n",
    "    full_notation = re.match(r'([A-G][#]?)\\s*(major|minor)$', key_str)\n",
    "    if full_notation:\n",
    "        note, scale = full_notation.groups()\n",
    "        note_idx = notes.index(note) if note in notes else -1\n",
    "        if note_idx >= 0:\n",
    "            if scale == 'major':\n",
    "                return note_idx\n",
    "            else:  # minor\n",
    "                return note_idx + 3 + 12  # Relative minor is 3 semitones up, then add 12 to differentiate\n",
    "    \n",
    "    # Case 2: Short notation with explicit minor (e.g., \"Dm\", \"F#m\")\n",
    "    short_minor = re.match(r'([A-G][#]?)m$', key_str)\n",
    "    if short_minor:\n",
    "        note = short_minor.group(1)\n",
    "        note_idx = notes.index(note) if note in notes else -1\n",
    "        if note_idx >= 0:\n",
    "            return note_idx + 3 + 12  # Minor key\n",
    "    \n",
    "    # Case 3: Just the note name - assume major (e.g., \"C\", \"F#\")\n",
    "    if key_str in notes:\n",
    "        return notes.index(key_str)  # Major key\n",
    "    \n",
    "    print(f\"Could not parse key: {key_str}\")\n",
    "    return np.nan\n",
    "\n",
    "# Function to load data and extract numeric features\n",
    "def load_data3(csv_filename):\n",
    "    # Read the CSV into a DataFrame\n",
    "    df = pd.read_csv(csv_filename)\n",
    "    \n",
    "    # Convert 'Estimated_Key' column to numerical data\n",
    "    df['Estimated_Key'] = df['Estimated_Key'].apply(key_to_number)\n",
    "    \n",
    "    # Exclude non-numeric columns (Title, Artist, and Album) and select the numeric columns\n",
    "    df_numeric = df.drop(columns=['Title', 'Artist', 'Album', 'song_id'])  # Exclude non-numeric and ID columns\n",
    "    \n",
    "    # Feature matrix with only numeric columns\n",
    "    feature_matrix = df_numeric.values\n",
    "    \n",
    "    return feature_matrix, df\n",
    "\n",
    "# Function for dimensional plane method with convex hull\n",
    "def dimensional_plane_method(feature_matrix, song_ids, pca_components=20):\n",
    "    \"\"\"\n",
    "    Finds songs within the convex hull formed by input songs in the feature space.\n",
    "    \"\"\"\n",
    "    # Apply PCA to reduce dimensionality\n",
    "    feature_matrix_reduced = apply_pca(feature_matrix, n_components=pca_components)\n",
    "    \n",
    "    points = feature_matrix_reduced[song_ids]  # Extract features of the input songs\n",
    "    print(points)\n",
    "    try:\n",
    "        hull = ConvexHull(points)\n",
    "    except QhullError:\n",
    "        print(\"Qhull precision error: points may be coplanar or nearly identical.\")\n",
    "        return []\n",
    "    \n",
    "    inside_songs = []\n",
    "    for i, song in enumerate(feature_matrix_reduced):\n",
    "        if i not in song_ids and all(np.dot(eq[:-1], song) + eq[-1] <= 0 for eq in hull.equations):\n",
    "            inside_songs.append(i)\n",
    "    \n",
    "    return inside_songs\n",
    "\n",
    "# Load the data\n",
    "data_numpy, df = load_data3(\"C:\\\\Users\\\\Cheig\\\\OneDrive\\\\Desktop\\\\csvs\\\\data_with_ids.csv\")\n",
    "x = data_numpy[~np.isnan(data_numpy)]\n",
    "print(np.count_nonzero(np.isnan(x)))\n",
    "print(x)\n",
    "# Example usage of the dimensional plane method with the first song (index 0)\n",
    "# You can specify any song_id(s) to be your input songs\n",
    "input_song_ids = range(200)  # Example song indices\n",
    "recommended_songs = dimensional_plane_method(x, input_song_ids)\n",
    "\n",
    "# Print recommended song IDs within the convex hull\n",
    "print(f\"Songs within the convex hull: {recommended_songs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f7a2e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m csv_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mCheig\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mcapstone\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mordered_songs_by_feature_with_values.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Change this to your actual CSV filename\u001b[39;00m\n\u001b[0;32m     35\u001b[0m song_id_to_search \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Change this to the song ID you're searching for\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m data \u001b[38;5;241m=\u001b[39m load_data(csv_filename)\n\u001b[0;32m     38\u001b[0m most_common_song \u001b[38;5;241m=\u001b[39m find_nearest_songs(data, song_id_to_search)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMost common nearby song: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmost_common_song\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 9\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(csv_filename)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load the CSV and parse the ordered song IDs with values using pandas.\"\"\"\u001b[39;00m\n\u001b[0;32m      8\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(csv_filename)\n\u001b[1;32m----> 9\u001b[0m data \u001b[38;5;241m=\u001b[39m {row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m\"\u001b[39m]: ast\u001b[38;5;241m.\u001b[39mliteral_eval(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mordered_song_ids_with_values\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows()}\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\Cheig\\anaconda3\\Lib\\ast.py:112\u001b[0m, in \u001b[0;36mliteral_eval\u001b[1;34m(node_or_string)\u001b[0m\n\u001b[0;32m    110\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m left \u001b[38;5;241m-\u001b[39m right\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _convert_signed_num(node)\n\u001b[1;32m--> 112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _convert(node_or_string)\n",
      "File \u001b[1;32mc:\\Users\\Cheig\\anaconda3\\Lib\\ast.py:86\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m operand\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _convert_num(node)\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert\u001b[39m(node):\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, Constant):\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m node\u001b[38;5;241m.\u001b[39mvalue\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import bisect\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# same as find_nearest_songs but uses a binary search \n",
    "def find_nearest_songs(data, song_id, num_neighbors=100):\n",
    "    \"\"\"Find the 100 nearest songs using binary search based on values.\"\"\"\n",
    "    song_counts = Counter()\n",
    "    \n",
    "    for feature, song_list in data.items():\n",
    "        song_ids = [song[0] for song in song_list]  # Extract song IDs\n",
    "        values = [song[1] for song in song_list]  # Extract values\n",
    "        \n",
    "        index = bisect.bisect_left(song_ids, song_id)  # Find position using binary search\n",
    "        if index < len(song_ids) and song_ids[index] == song_id:\n",
    "            nearest_songs = song_ids[max(0, index - num_neighbors // 2): index + num_neighbors // 2]\n",
    "            song_counts.update(nearest_songs)\n",
    "    \n",
    "    # Remove the original song ID itself from the count\n",
    "    song_counts.pop(song_id, None)\n",
    "    \n",
    "    # Find the most common nearby song\n",
    "    most_common_song= song_counts.most_common(10) if song_counts else (None, 0)\n",
    "    \n",
    "    return most_common_song,\n",
    "\n",
    "# Example usage\n",
    "csv_filename = \"C:\\\\Users\\\\Cheig\\\\capstone\\\\ordered_songs_by_feature_with_values.csv\"  # Change this to your actual CSV filename\n",
    "song_id_to_search = 1  # Change this to the song ID you're searching for\n",
    "\n",
    "data = load_data(csv_filename)\n",
    "most_common_song = find_nearest_songs(data, song_id_to_search)\n",
    "print(f\"Most common nearby song: {most_common_song}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2397b87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "distance_matrix = pd.read_csv(\"distances2.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47384a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('90', 5.795593522907614), ('75', 6.005682414950607), ('24497', 6.02461700560429), ('189', 6.239342590205207), ('45212', 6.313814136778167), ('15733', 6.330860637777792), ('98', 6.393084541841931), ('45255', 6.451410435094632), ('107', 6.464909964167343), ('25828', 6.487933809127037)]\n",
      "Top 10 songs most similar to 100:\n",
      "1. Song ID: 90, Distance: 5.7956\n",
      "2. Song ID: 75, Distance: 6.0057\n",
      "3. Song ID: 24497, Distance: 6.0246\n",
      "4. Song ID: 189, Distance: 6.2393\n",
      "5. Song ID: 45212, Distance: 6.3138\n",
      "6. Song ID: 15733, Distance: 6.3309\n",
      "7. Song ID: 98, Distance: 6.3931\n",
      "8. Song ID: 45255, Distance: 6.4514\n",
      "9. Song ID: 107, Distance: 6.4649\n",
      "10. Song ID: 25828, Distance: 6.4879\n"
     ]
    }
   ],
   "source": [
    "def find_similar_songs(song_id, n=10):\n",
    "    \"\"\"\n",
    "    Find the n most similar songs to the given song_id.\n",
    "    \n",
    "    Parameters:\n",
    "    csv_file (str): Path to the CSV file containing the distance matrix\n",
    "    song_id (str): ID of the song to find similar songs for\n",
    "    n (int): Number of similar songs to return (default: 10)\n",
    "    \n",
    "    Returns:\n",
    "    list: List of tuples (song_id, distance) for the n most similar songs\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the song_id exists in the dataset\n",
    "    if song_id not in distance_matrix.index:\n",
    "        return f\"Song ID '{song_id}' not found in the dataset.\"\n",
    "    \n",
    "    # Get the row for the specified song_id\n",
    "    distances = distance_matrix.loc[song_id]\n",
    "    \n",
    "    # Sort the distances in ascending order (closest first)\n",
    "    # Exclude the song itself (distance = 0)\n",
    "    sorted_distances = distances.sort_values()\n",
    "    \n",
    "    # Remove the song itself (will be at index 0 with distance 0)\n",
    "    if sorted_distances.index[0] == song_id or sorted_distances.iloc[0] == 0:\n",
    "        sorted_distances = sorted_distances.iloc[1:]\n",
    "    \n",
    "    # Return the top n similar songs with their distances\n",
    "    similar_songs = [(idx, dist) for idx, dist in \n",
    "                     zip(sorted_distances.index[:n], sorted_distances.values[:n])]\n",
    "    \n",
    "    return similar_songs\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your actual CSV file path and song ID\n",
    "    song_id = 100  # Replace with your actual song ID\n",
    "    \n",
    "    similar_songs = find_similar_songs(song_id)\n",
    "    print(similar_songs)\n",
    "    print(f\"Top 10 songs most similar to {song_id}:\")\n",
    "    for i, (similar_id, distance) in enumerate(similar_songs, 1):\n",
    "        print(f\"{i}. Song ID: {similar_id}, Distance: {distance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ceab97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea854f13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd489bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11222,TE MATA,Feid,INTER SHIBUYA (FERXXO EDITION),113.12,C major,0.6812122948850745,1328.3975830078125,966.456787109375,3853.69873046875,138.19027210595792,84.46632711250275,553.263656988684,0.5814168377823409,0.1727958024910659,0.0518779902587347,5.78714412294581,0.1166161075234413,112.34714673913044,18.640080416880373,1.60007072135785,1.4009597301483154,27.36082000793177,3485.3446391149887,1872.8492680223808,7551.129045157597,4018.727730383597,3637.869828118626,1465.2302040459315,22.47957206700756,14.476959087449586,1.0,0.0038064462132751,0.0933149676440971,0.0302356409083095,0.0056167947021982,0.0567645261216662,0.0015421378393131,-0.0016682281797917,2.1284595522415648e-10,-4.88952000080811e-07,0.0003821593249041,-0.118290635394755,13.021915441176535,0.2083112150430679,0.1574039012193679,0.7005082368850708,3.829923152923584,0.2144597433609426,0.3889042381110932,0.1166161075234413,0.0660918429493904,1.4765149354934692,0.463833212852478,0.7566723227500916,1.87850284576416,15.738718032836914,28.63440704345703,7.955277919769287,0.8048398494720459,2.2703897953033447,2.6151180267333984,1.878843903541565,0.0006711782189086,0.3909777402877807,1.7644551992416382,6.8597586647288304,3.6686704384724185,8.972517013549805,7.954811573028564,66.10935974121094,23.62055065639309,3853.69873046875,0.0010338589781895,6.771565437316895,4018.727730383597,17.8016482388115,3936.635731234289,1.5903602838516235,0.4528567194938659,0.315658688545227,0.0675575494374743,0.0593171708374753,-152.3648223876953,124.03955841064452,0.0017667779466137,17.171310424804688,-0.000136573522468,8.601082801818848,101.00070190429688,40.35942459106445,0.0023829315323382,6.331023216247559,-0.0001801710459403,3.1233832836151123,-2.255601644515991,27.75527000427246,0.0020694143604487,3.9666152000427246,-0.0001490545837441,2.1406805515289307,18.772838592529297,39.32360076904297,0.0016439517494291,3.7890758514404297,-0.0001191088958876,1.7728251218795776,-22.29355239868164,20.417707443237305,0.0012080881278961,2.93129825592041,-0.0001107057905755,1.4450770616531372,16.74798583984375,25.60814094543457,0.0008391875890083,3.258012294769287,-0.0001337691501248,1.4639378786087036,-14.075129508972168,16.931488037109375,0.0005668885423801,2.519221305847168,-0.0001810444664442,1.2747445106506348,6.0478339195251465,19.586584091186523,0.0003759019309654,2.070576906204224,-0.0002298439067089,1.051316499710083,-7.116996765136719,15.292885780334473,0.0002303831133758,2.0137338638305664,-0.0002517513639759,1.0083341598510742,7.007184028625488,15.796504974365234,0.0001040072675095,1.9249494075775144,-0.0002258046297356,0.9648210406303406,-5.912801742553711,13.425474166870115,-2.128345840901602e-06,1.938229322433472,-0.0001486489200033,0.9938627481460572,3.2243220806121826,14.443333625793455,-6.320431566564366e-05,1.7277512550354004,-3.656218905234709e-05,0.8730620741844177,-5.689050197601318,12.44824504852295,-5.204121771384962e-05,1.8441452980041504,8.17062464193441e-05,0.9208766222000122,3.6686704384724185,1.4009597301483154,2.571836233139038,23.823392868041992,29.28634262084961,10.129961234738564,0.1383101940155029,0.1166161075234413,0.0660918429493904,1.7644553184509275,415.0,0.2668892613406071,0.1457777132225021,0.4760090702947845,110.96816326530612,14.530141830444336,0.3754023015499115,0.3911081658652714,0.8046354651451111,,,,,,,,,,,,,,\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import argparse\n",
    "\n",
    "def find_song_by_id(csv_file, song_id):\n",
    "    try:\n",
    "        with open(csv_file, 'r') as file:\n",
    "            for line in file:\n",
    "                if line.startswith(str(song_id) + \",\"):\n",
    "                    print(line.strip())\n",
    "                    return\n",
    "        print(f\"No song found with song_id: {song_id}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{csv_file}' was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    find_song_by_id(\"C:\\\\Users\\\\Cheig\\\\OneDrive\\\\Desktop\\\\csvs\\\\data_with_ids.csv\", 11222)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2604946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eeeee\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmy_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     30\u001b[0m row_to_read \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m---> 31\u001b[0m data \u001b[38;5;241m=\u001b[39m read_specific_row(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistances2.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m11111\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRow \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow_to_read\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 19\u001b[0m, in \u001b[0;36mread_specific_row\u001b[1;34m(filename, row_number)\u001b[0m\n\u001b[0;32m     17\u001b[0m reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mreader(file)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meeeee\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(reader)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meeeee\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m row_number \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(rows):\n",
      "File \u001b[1;32m<frozen codecs>:319\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def read_specific_row(filename, row_number):\n",
    "    \"\"\"Reads a specific row from a CSV file.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The path to the CSV file.\n",
    "        row_number (int): The row number to read (starting from 0).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of strings representing the row, or None if the row \n",
    "              number is invalid.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            \n",
    "            reader = csv.reader(file)\n",
    "            print(\"eeeee\")\n",
    "            rows = list(reader)\n",
    "            print(\"eeeee\")\n",
    "            if 0 <= row_number < len(rows):\n",
    "                return rows[row_number]\n",
    "            else:\n",
    "                return None\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "    \n",
    "# Example usage\n",
    "filename = 'my_data.csv'\n",
    "row_to_read = 2\n",
    "data = read_specific_row(\"distances2.csv\", 11111)\n",
    "\n",
    "if data:\n",
    "    print(f\"Row {row_to_read}: {data}\")\n",
    "else:\n",
    "    print(f\"Row {row_to_read} not found or file does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286bdceb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
